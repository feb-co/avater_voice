{
    "architectures": [
      "LlamaTTSForCausalLM"
    ],
    "auto_map": {
      "AutoConfig": "configuration_llama_tts.LlamaTTSConfig",
      "AutoModel": "modeling_llama_tts.LlamaTTSForCausalLM",
      "AutoModelForCausalLM": "modeling_llama_tts.LlamaTTSForCausalLM"
    },
    "audio_special_tokens": 8,
    "block_step": 1,
    "code_size": 2048,
    "code_layers": 8,
    "tts_adapter_hidden_layers": 12,
    "tts_adapter_hidden_size": 1280,
    "tts_adapter_intermediate_size": 4480,
    "tts_adapter_attention_heads": 20,
    "tts_adapter_dropout": 0.1,
    "tts_adapter_attention_dropout": 0.0,
    "boa_token_id": 2048,
    "eoa_token_id": 2049,
    "tie_audio_embeddings": false,
    "torch_dtype": "bfloat16"
}