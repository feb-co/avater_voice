{
    "architectures": [
      "LlamaTTSForCausalLM"
    ],
    "auto_map": {
      "AutoConfig": "configuration_llama_tts.LlamaTTSConfig",
      "AutoModel": "modeling_llama_tts.LlamaTTSForCausalLM",
      "AutoModelForCausalLM": "modeling_llama_tts.LlamaTTSForCausalLM"
    },
    "audio_special_tokens": 8,
    "code_size": 2048,
    "code_layers": 8,
    "tts_adapter_hidden_layers": 12,
    "tts_adapter_hidden_size": 768,
    "tts_adapter_intermediate_size": 2688,
    "tts_adapter_attention_heads": 12,
    "tts_adapter_dropout": 0.1,
    "tts_adapter_attention_dropout": 0.0,
    "boa_token_id": 2048,
    "eoa_token_id": 2049,
    "llm_path": "/mnt/ceph/huggingface/Llama-3.1-8B",
    "tie_audio_embeddings": true
}